{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jMDM_8UF7jW_"
   },
   "source": [
    "# Question 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from random import seed\n",
    "from random import randrange\n",
    "from random import random\n",
    "from random import *\n",
    "from csv import reader\n",
    "from math import exp\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm\n",
    "import pylab as py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a) Import data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dataset directory path**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For Windows use 'r' in front of the string\n",
    "path = \"/Users/lunguteodor/OneDrive/Documents/Facultate/2021-2022 (Third Year)/Quarter 2 (2021-2022)/Data Mining/2IIG0---Data-Mining-and-Machine-Learning/HW2/data/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(path + \"train_data.csv\")\n",
    "validate_data = pd.read_csv(path + \"validate_data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explore dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>X_1</th>\n",
       "      <th>X_2</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2917.211242</td>\n",
       "      <td>3289.522533</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1888.937716</td>\n",
       "      <td>781.528356</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4188.521414</td>\n",
       "      <td>1554.476261</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8145.555339</td>\n",
       "      <td>9804.066728</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9584.488981</td>\n",
       "      <td>6176.337189</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           X_1          X_2  y\n",
       "0  2917.211242  3289.522533  0\n",
       "1  1888.937716   781.528356  0\n",
       "2  4188.521414  1554.476261  0\n",
       "3  8145.555339  9804.066728  0\n",
       "4  9584.488981  6176.337189  0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>X_1</th>\n",
       "      <th>X_2</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>410.000000</td>\n",
       "      <td>410.000000</td>\n",
       "      <td>410.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>4895.705237</td>\n",
       "      <td>4973.234806</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>2860.877356</td>\n",
       "      <td>3119.801712</td>\n",
       "      <td>0.500611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>27.557777</td>\n",
       "      <td>29.069434</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>2341.140941</td>\n",
       "      <td>2124.504836</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>4987.225191</td>\n",
       "      <td>5701.130455</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>7303.607081</td>\n",
       "      <td>7762.583984</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>9941.145899</td>\n",
       "      <td>9960.403013</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               X_1          X_2           y\n",
       "count   410.000000   410.000000  410.000000\n",
       "mean   4895.705237  4973.234806    0.500000\n",
       "std    2860.877356  3119.801712    0.500611\n",
       "min      27.557777    29.069434    0.000000\n",
       "25%    2341.140941  2124.504836    0.000000\n",
       "50%    4987.225191  5701.130455    0.500000\n",
       "75%    7303.607081  7762.583984    1.000000\n",
       "max    9941.145899  9960.403013    1.000000"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b) Data normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use minmax of data normalization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c) Size of input/output layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since there are 2 features, we'll have 2 inputs, hence 2x10 connections with the first hidden layer. Because there are 2 classes, then we'll have two neurons in the last layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### d) Loss function & activation function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We choose for activation we can use either ReLU or sigmoid."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### e) Initialization of the parameter values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We initialize the weights randomly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### f) Implement MLP & g) Tune hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load data into a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_csv_file(path):\n",
    "\tdata = list()\n",
    "\twith open(path, 'r') as file:\n",
    "\t\tfile_reader = reader(file)\n",
    "\n",
    "\t\tfor i in file_reader:\n",
    "\t\t\tif not i:\n",
    "\t\t\t\tcontinue\n",
    "\n",
    "\t\t\tdata.append(i)\n",
    "\n",
    "\treturn data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For the feature columns: convert string to float\n",
    "def convert_float(data, feature):\n",
    "\tfor line in data:\n",
    "\t\tline[feature] = float(line[feature].strip())\n",
    "\n",
    "#For the classes column: convert string to int\n",
    "def convert_int(data, feature):\n",
    "\tfor line in data:\n",
    "\t\tline[feature] = int(line[feature].strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalize data with MinMax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_data_minmax(data):\n",
    "\tmin_and_max = [[min(feature), max(feature)] for feature in zip(*data)]\n",
    "\tfor obs in data:\n",
    "\t\tfor i in range(len(obs)-1):\n",
    "\t\t\tobs[i] = (obs[i] - min_and_max[i][0]) / (min_and_max[i][1] - min_and_max[i][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load, convert and normalize a dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(path):\n",
    "\tdatafile = path\n",
    "\tdata = load_csv_file(datafile)\n",
    "\tdata = data[1:]\n",
    "\tfor i in range(len(data[0])-1):\n",
    "\t\tconvert_float(data, i)\n",
    "\tconvert_int(data, len(data[0])-1)\n",
    "\tnormalize_data_minmax(data)\n",
    "\treturn data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate accuracy of a neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_accuracy(trainset, validationset, algorithm, *args):\n",
    "\toutputs = algorithm(trainset, validationset, *args)\n",
    "\tgroundtruth = [line[-1] for line in validationset]\n",
    "\taccuracy = compute_accuracy(groundtruth, outputs[0])\n",
    "\treturn accuracy, outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy(groundtruth, outputs):\n",
    "\tcounter = 0\n",
    "\tfor line in range(len(groundtruth)):\n",
    "\t\tif groundtruth[line] == outputs[line]:\n",
    "\t\t\tcounter += 1\n",
    "\treturn counter / float(len(groundtruth)) * 100.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Declare, define, run and collect outputs of a neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def algorithm(trainset, validationset, lr, epochs, n_hidden, batches):\n",
    "    #Number of I/O\n",
    "\tinputs = len(trainset[0]) - 1\n",
    "\toutputs = len(set([line[-1] for line in trainset]))\n",
    "\n",
    "    #Retrieve neural network\n",
    "\tnetwork = init_neuralnet(inputs, n_hidden, outputs)\n",
    "\terrors = train_network(network, trainset, lr, epochs, outputs, batches)\n",
    "\n",
    "    #Run on validation set\n",
    "\tvalidation_results = list()\n",
    "\tfor line in validationset:\n",
    "\t\tguess = classify_observation(network, line)\n",
    "\t\tvalidation_results.append(guess)\n",
    "\t\t\n",
    "\treturn validation_results, errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicts the class of an observation\n",
    "# Pass forward and pick the best prediction\n",
    "def classify_observation(network, row):\n",
    "\toutputs = forward_pass(network, row)\n",
    "\treturn outputs.index(max(outputs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize network with 2 hidden layers and an output layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_neuralnet(nrof_inputs, nrof_neurons_hidden, nrof_outputs):\n",
    "\tneuralnet = list()\n",
    "\thidden_layer_first = [{'weights':[random() for i in range(nrof_inputs + 1)]} for i in range(nrof_neurons_hidden)]\n",
    "\tneuralnet.append(hidden_layer_first)\n",
    "\n",
    "\thidden_layer_second = [{'weights':[random() for i in range(nrof_neurons_hidden + 1)]} for i in range(nrof_neurons_hidden)]\n",
    "\tneuralnet.append(hidden_layer_second)\n",
    "\n",
    "\toutput_layer = [{'weights':[random() for i in range(nrof_neurons_hidden + 1)]} for i in range(nrof_outputs)]\n",
    "\tneuralnet.append(output_layer)\n",
    "\n",
    "\treturn neuralnet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train without batches, only SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_network_SGD(network, training_dataset, learning_rate, nrof_epochs, nrof_outputs, dummy):\n",
    "\terrors = list()\n",
    "\n",
    "\tfor epoch in range(nrof_epochs):\n",
    "\t\terror = 0\n",
    "\n",
    "\t\tfor observation in training_dataset:\n",
    "\t\t\toutputs = forward_pass(network, observation)\n",
    "\t\t\tgroundtruths = [0 for i in range(nrof_outputs)]\n",
    "\t\t\tgroundtruths[observation[-1]] = 1\n",
    "\t\t\terror += sum([(groundtruths[i] - outputs[i]) ** 2 for i in range(len(groundtruths))])\n",
    "\t\t\t\n",
    "\t\t\tbackward_pass(network, groundtruths)\n",
    "\t\t\tupdate_weights(network, observation, learning_rate)\n",
    "\n",
    "\t\tprint('>epoch=%d, error=%.3f' % (epoch, error))\n",
    "\t\terrors.append(error)\n",
    "\t\n",
    "\treturn errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train with batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_network(neuralnet, train, learning_rate, nrof_epochs, nrof_outputs, batch_size):\n",
    "\terrors = list()\n",
    "\tfor epoch in range(nrof_epochs):\n",
    "\t\tbatch_error = 0\n",
    "\t\tfor batch in make_batches(train, batch_size=batch_size):\n",
    "\t\t\terror = 0\n",
    "\t\t\tfor line in batch:\n",
    "\t\t\t\toutputs = forward_pass(neuralnet, line)\n",
    "\t\t\t\tgroundtruths = [0 for i in range(nrof_outputs)]\n",
    "\t\t\t\tgroundtruths[line[-1]] = 1\n",
    "\t\t\t\terror += sum([(groundtruths[i] - outputs[i]) ** 2 for i in range(len(groundtruths))])\n",
    "\t\t\t\tbackward_pass(neuralnet, groundtruths)\n",
    "\t\t\t\t\n",
    "\t\t\tfor row in batch:\n",
    "\t\t\t\tupdate_weights(neuralnet, row, learning_rate)\n",
    "\t\t\tbatch_error += error\n",
    "\t\terrors.append(batch_error/batch_size)\n",
    "\t\tprint(epoch)\n",
    "\t\n",
    "\treturn errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_batches(iterable, batch_size=1):\n",
    "    n = len(iterable)\n",
    "    for i in range(0, n, batch_size):\n",
    "        yield iterable[i:min(i + batch_size, n)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Forward pass, backward pass, update weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_pass(neuralnet, row):\n",
    "\tinputs = row\n",
    "\tfor layer in neuralnet:\n",
    "\t\tnew_inputs = []\n",
    "\t\tfor neuron in layer:\n",
    "\t\t\tneuron['output'] = activation(sum_weight_input(neuron['weights'], inputs))\n",
    "\t\t\tnew_inputs.append(neuron['output'])\n",
    "\t\tinputs = new_inputs\n",
    "\treturn inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_pass(neuralnet, expected):\n",
    "\tfor i in reversed(range(len(neuralnet))):\n",
    "\t\tlayer = neuralnet[i]\n",
    "\t\terrors = list()\n",
    "\t\tif i != len(neuralnet)-1:\n",
    "\t\t\tfor j in range(len(layer)):\n",
    "\t\t\t\terror = 0.0\n",
    "\t\t\t\tfor neuron in neuralnet[i + 1]:\n",
    "\t\t\t\t\terror += (neuron['weights'][j] * neuron['weight_change'])\n",
    "\t\t\t\terrors.append(error)\n",
    "\t\telse:\n",
    "\t\t\tfor j in range(len(layer)):\n",
    "\t\t\t\tneuron = layer[j]\n",
    "\t\t\t\terrors.append(neuron['output'] - expected[j])\n",
    "\t\tfor j in range(len(layer)):\n",
    "\t\t\tneuron = layer[j]\n",
    "\t\t\tneuron['weight_change'] = errors[j] * activation_derivative(neuron['output'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_weights(neuralnet, line, l_rate):\n",
    "\tfor i in range(len(neuralnet)):\n",
    "\t\tinputs = line[:-1]\n",
    "\t\t\n",
    "\t\tif i != 0:\n",
    "\t\t\tinputs = [neuron['output'] for neuron in neuralnet[i - 1]]\n",
    "\n",
    "\t\tfor neuron in neuralnet[i]:\n",
    "\t\t\tfor j in range(len(inputs)):\n",
    "\t\t\t\tneuron['weights'][j] -= l_rate * neuron['weight_change'] * inputs[j]\n",
    "\t\t\tneuron['weights'][-1] -= l_rate * neuron['weight_change']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Activation sum of weights * input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sum_weight_input(weights, inputs):\n",
    "\ty = weights[-1]\n",
    "\tfor i in range(len(weights)-1):\n",
    "\t\ty += weights[i] * inputs[i]\n",
    "\treturn y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Activation function: ReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def activation(swi):\n",
    "\treturn max(0, swi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def activation_derivative(output):\n",
    "    if output <= 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = '/Users/lunguteodor/OneDrive/Documents/Facultate/2021-2022 (Third Year)/Quarter 2 (2021-2022)/Data Mining/2IIG0---Data-Mining-and-Machine-Learning/HW2/data/train_data.csv'\n",
    "validate_path = '/Users/lunguteodor/OneDrive/Documents/Facultate/2021-2022 (Third Year)/Quarter 2 (2021-2022)/Data Mining/2IIG0---Data-Mining-and-Machine-Learning/HW2/data/validate_data.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0.29148411546616415, 0.3282996259301394, 0],\n",
       " [0.18776046734102808, 0.07576615122002546, 0],\n",
       " [0.41972327129796977, 0.15359536711078225, 0],\n",
       " [0.8188758159059557, 0.9842582787339653, 0],\n",
       " [0.9640234278831104, 0.618977069502859, 0],\n",
       " [0.4047047275405339, 0.013949557474687862, 0],\n",
       " [0.23921338819416202, 0.6265847570706259, 1],\n",
       " [0.5923505178588698, 0.7756707880883303, 0],\n",
       " [0.1820692960610651, 0.12804532501597507, 0],\n",
       " [0.1598544521221301, 0.6441946872587039, 1],\n",
       " [0.9339775979231282, 0.15444922112067727, 1],\n",
       " [0.8543409374826657, 0.8837796383714998, 0],\n",
       " [0.9408962572579236, 0.9884350286810096, 0],\n",
       " [0.6992537188979283, 0.7115794899822552, 1],\n",
       " [0.836348688681416, 0.657758424487849, 0],\n",
       " [0.2954324980819442, 0.17272054063253303, 0],\n",
       " [0.34969811577677634, 0.7196033403913101, 1],\n",
       " [0.9790650339260872, 0.01947117873169356, 1],\n",
       " [0.3358849324653488, 0.9170507088045807, 1],\n",
       " [0.14517816093342345, 0.7661268085969148, 0],\n",
       " [0.23015567035304924, 0.28822853136504534, 0],\n",
       " [0.16273924052051011, 0.9840370526536476, 1],\n",
       " [0.5804153563868927, 0.022095261478769005, 1],\n",
       " [0.3833621677729586, 0.20226708840147106, 0],\n",
       " [0.09316885934989715, 0.26197824182162427, 0],\n",
       " [0.6981606220426935, 0.7725448766733952, 0],\n",
       " [0.7231171768199386, 0.9279410880411751, 0],\n",
       " [0.08281697999607977, 0.13012316650986244, 0],\n",
       " [0.04902288733407956, 0.914485285359941, 1],\n",
       " [0.3146482819797715, 0.58763414828033, 1],\n",
       " [0.8022847387924319, 0.6138408685491356, 0],\n",
       " [0.11061499038495574, 0.8414375762852078, 1],\n",
       " [0.7635714582641425, 0.08244687927859783, 1],\n",
       " [0.7087165933736181, 0.8175673679073291, 0],\n",
       " [0.7484664863649, 0.20449120138935772, 1],\n",
       " [0.8912044982349507, 0.5953735993207057, 0],\n",
       " [0.1625285870240819, 0.6514418486220559, 1],\n",
       " [0.29552935393153223, 0.41138870771752334, 0],\n",
       " [0.15012359425159186, 0.6132088269456496, 1],\n",
       " [0.22990910554019076, 0.44545618680379434, 0],\n",
       " [0.2896225368153956, 0.5693686226530058, 1],\n",
       " [0.6277460510089146, 0.5825648483117278, 0],\n",
       " [0.03584623443146962, 0.8251986639862929, 1],\n",
       " [0.6506546472921594, 0.21943303732913283, 1],\n",
       " [0.4457147524685628, 0.8806857353467241, 1],\n",
       " [0.5682782350417185, 0.27580424241945933, 1],\n",
       " [0.926955662056423, 0.2964081613567415, 1],\n",
       " [0.10841085221302424, 0.888537462849383, 1],\n",
       " [0.1410438014566569, 0.2958589368284711, 0],\n",
       " [0.7982819253294782, 0.22671564589574328, 1],\n",
       " [0.3770479170464695, 0.4242520213887753, 0],\n",
       " [0.9101895764617428, 0.8790606663797333, 0],\n",
       " [0.03731937600449198, 0.07550463771034915, 0],\n",
       " [0.4163627373933141, 0.9622287269864033, 1],\n",
       " [0.6156231809114486, 0.7254329466108268, 0],\n",
       " [0.6865402297503181, 0.42834928241384124, 1],\n",
       " [0.25346332043512115, 0.7828679310935183, 1],\n",
       " [0.557399983123709, 0.1948565799924174, 1],\n",
       " [0.7698577614910385, 0.1016460144988617, 1],\n",
       " [0.6813143489326319, 0.023955355186540177, 1],\n",
       " [0.579135845593808, 0.7397890596007332, 0],\n",
       " [0.2622289190188643, 0.5638360595220678, 0],\n",
       " [0.737043915181886, 0.9391121394531465, 0],\n",
       " [0.43092904580014524, 0.9703185321833819, 1],\n",
       " [0.8956701036702607, 0.15509706815439595, 1],\n",
       " [0.4018079957369462, 0.04077289097587676, 0],\n",
       " [0.8256317069291994, 0.08467012480997109, 1],\n",
       " [0.09935527133783538, 0.1054775327634999, 0],\n",
       " [0.7835535526952184, 0.24292959850540508, 1],\n",
       " [0.3121678331555965, 0.7182107380897119, 1],\n",
       " [0.09416500743257912, 0.7108000526651561, 1],\n",
       " [0.43690438180083274, 0.3381063579491236, 0],\n",
       " [0.6209105586354529, 0.8025018706294028, 0],\n",
       " [0.4164182762147011, 0.755678221589442, 1],\n",
       " [0.5742459844855432, 0.3845216410865844, 1],\n",
       " [0.3250614377143038, 0.7748938057285446, 1],\n",
       " [0.09007437909479826, 0.7779346380356961, 1],\n",
       " [0.3291164629489836, 0.6099024160050719, 1],\n",
       " [0.5761303410646689, 0.34261082793236675, 1],\n",
       " [0.3815171110916312, 0.5801798231978618, 1],\n",
       " [0.9833587113995871, 0.2132258746646344, 1],\n",
       " [0.7197994348887631, 0.26822883843148065, 1],\n",
       " [0.9231015309859485, 0.0007932234304050994, 1],\n",
       " [0.30065644801448926, 0.8424289116302272, 1],\n",
       " [0.05472321172781788, 0.6109075404348009, 1],\n",
       " [0.18596061719788798, 0.8590597490386002, 1],\n",
       " [0.993590093594825, 0.5739960492352082, 0],\n",
       " [0.6220499160367551, 0.36265262407358984, 1],\n",
       " [0.18266670326895526, 0.923079126088523, 1],\n",
       " [0.8369540763499655, 0.2920109593442628, 1],\n",
       " [0.6910595090919336, 0.8304342375964833, 0],\n",
       " [0.7491361089998593, 0.7520152217807285, 1],\n",
       " [0.07916756272600455, 0.29069266247145037, 0],\n",
       " [0.9686417657076825, 0.2868101358506863, 1],\n",
       " [0.4339714208342429, 0.8973844895151817, 1],\n",
       " [0.16733930705947156, 0.9422155572121972, 1],\n",
       " [0.4040642006271698, 0.6187100877346081, 1],\n",
       " [0.21243121440009927, 0.17760837242411034, 0],\n",
       " [0.6562841189137861, 0.7531575085551889, 0],\n",
       " [0.30460731237549593, 0.17994829755251723, 0],\n",
       " [0.38907069059081967, 0.39963017115528365, 0],\n",
       " [0.7635317997676253, 0.9734808720393844, 0],\n",
       " [0.13891324885142836, 0.10889658396443196, 0],\n",
       " [0.25908708080474663, 0.2554285115680956, 0],\n",
       " [0.736321781085526, 0.1237213311979669, 1],\n",
       " [0.8668776889064086, 0.693639110517259, 0],\n",
       " [0.10844603906925786, 0.21024784057033538, 0],\n",
       " [0.9865688062319633, 0.5530611518876385, 0],\n",
       " [0.4061907774771505, 0.09803590436461378, 0],\n",
       " [0.8774736012753446, 0.3537836163718846, 1],\n",
       " [0.8134645756626581, 0.20750264529688703, 1],\n",
       " [0.6403409100515058, 0.37077317247313774, 1],\n",
       " [0.8134094061323849, 0.8944446395174074, 0],\n",
       " [0.5529368089978003, 0.8425107280341912, 0],\n",
       " [0.9719621838646276, 0.7409394183022275, 0],\n",
       " [0.14049176914173933, 0.5882129721567305, 1],\n",
       " [0.30889274813076056, 0.3317055061909394, 0],\n",
       " [0.8403862567655793, 0.8974863286077827, 0],\n",
       " [0.39370276874818827, 0.8625474027081316, 1],\n",
       " [0.31972550935514216, 0.26314288500957483, 0],\n",
       " [0.011708806382131138, 0.9911279459803196, 1],\n",
       " [0.30126924642061426, 0.6140373602875798, 1],\n",
       " [0.5735233136511289, 0.07754116423915779, 1],\n",
       " [0.24952530621459193, 0.015310845220438492, 0],\n",
       " [0.7358538909590626, 0.003116725930501007, 1],\n",
       " [0.29925589699003124, 0.4476039002836021, 0],\n",
       " [0.71172321011358, 0.16054521845262268, 1],\n",
       " [0.7265894764128992, 0.798928291339388, 0],\n",
       " [0.6388130714108488, 0.8804191789996885, 0],\n",
       " [0.6546600961278641, 0.39029790653411023, 1],\n",
       " [0.7384507808725609, 0.2362689466932315, 1],\n",
       " [0.6978106299956787, 0.18234699273384403, 1],\n",
       " [0.5799389546096564, 0.03430831548965861, 1],\n",
       " [0.22210473783299312, 0.8570002052887726, 1],\n",
       " [0.3867253105211211, 0.2543230092795349, 0],\n",
       " [0.4403422020513189, 0.6425463970396864, 1],\n",
       " [0.3373485585331029, 0.6447241782840724, 1],\n",
       " [0.18744536689892477, 0.7948628551440379, 1],\n",
       " [0.31757448666405697, 0.22128475823386215, 0],\n",
       " [0.9166112560007788, 0.21922442959447885, 1],\n",
       " [0.8308983403982, 0.6608800743401588, 0],\n",
       " [0.1832998326589303, 0.300962738507974, 0],\n",
       " [0.32843257635648826, 0.8001923998199043, 1],\n",
       " [0.23528505028848687, 0.3581598229159962, 0],\n",
       " [0.37110427249756933, 0.9115777240772172, 1],\n",
       " [0.42645320562452194, 0.9101580588441542, 1],\n",
       " [0.4481033066135551, 0.7437819295093635, 1],\n",
       " [0.3517132242072074, 0.8043217374032156, 1],\n",
       " [0.8277484330572416, 0.39799404053185716, 1],\n",
       " [0.33375530505490214, 0.20936979132038627, 0],\n",
       " [0.44616945019703724, 0.1557871389233931, 0],\n",
       " [0.772247201995227, 0.9750337465730393, 0],\n",
       " [0.14250015891499182, 0.9263224699701785, 1],\n",
       " [0.6814723753687474, 0.9888213874685271, 0],\n",
       " [0.6398013341750123, 0.8374189512251704, 0],\n",
       " [0.16419470738303937, 0.14965765948178372, 0],\n",
       " [0.5940653470197332, 0.7191729407912952, 0],\n",
       " [0.8997573289515916, 0.1198998368640736, 1],\n",
       " [0.9388365000086629, 0.6730269615674263, 0],\n",
       " [0.8730966075504677, 0.7555317369317237, 0],\n",
       " [0.9787769165497231, 0.10321550341755253, 1],\n",
       " [0.6170939323508776, 0.7060761144722641, 0],\n",
       " [0.01626803847295661, 0.4313491371426277, 0],\n",
       " [0.35111479839738663, 0.34099976292555867, 0],\n",
       " [0.3293303924540175, 0.18215262457727646, 0],\n",
       " [0.1886636550557101, 0.0916250420777116, 0],\n",
       " [0.7470263229422692, 0.06350070998481563, 1],\n",
       " [0.4191696750689769, 0.2595327218109528, 0],\n",
       " [0.3512122199351928, 0.2611055023318983, 0],\n",
       " [0.26818748732676906, 0.3459715055024676, 0],\n",
       " [0.10832815309440746, 0.9000447226842668, 1],\n",
       " [0.8944616780373971, 0.2713666639563447, 1],\n",
       " [0.4239047310772972, 0.2187229336009749, 0],\n",
       " [0.5879793423099442, 0.8817513758184549, 0],\n",
       " [0.8499861607388954, 0.9777114556428805, 0],\n",
       " [0.5773927184043647, 0.9925666680699965, 0],\n",
       " [0.9584807344279187, 0.6170594008586049, 1],\n",
       " [0.5854693156985545, 0.6478611697819717, 0],\n",
       " [0.8354935593482349, 0.7142335465387524, 0],\n",
       " [0.5769370994151028, 0.7572088797713609, 0],\n",
       " [0.03450493306442979, 0.3168029267112258, 0],\n",
       " [0.6802196894746533, 0.05088256303570038, 1],\n",
       " [0.8973252880291879, 0.7286652284332809, 0],\n",
       " [1.0, 0.14093492629482016, 1],\n",
       " [0.19092103015795556, 0.1370562348085521, 0],\n",
       " [0.5564314557971687, 0.22882293872138532, 1],\n",
       " [0.870813473359095, 0.010404482100864107, 1],\n",
       " [0.8926601036951283, 0.781847401985376, 0],\n",
       " [0.5840789251728749, 0.916386504048235, 0],\n",
       " [0.6357966866637021, 0.6161446940946265, 0],\n",
       " [0.1768488561593557, 0.24631571072614888, 0],\n",
       " [0.7559145745942119, 0.9582319055440047, 0],\n",
       " [0.2049954573266211, 0.5840120830547486, 1],\n",
       " [0.8748961975456203, 0.6601513731096079, 0],\n",
       " [0.3943596223434774, 0.179131169627503, 0],\n",
       " [0.1601502636854086, 1.0, 1],\n",
       " [0.606115775233274, 0.4051727089788661, 1],\n",
       " [0.6907415364304431, 0.23998500402899414, 1],\n",
       " [0.11746234294451317, 0.7254945038020277, 1],\n",
       " [0.1136188838929587, 0.877215375527876, 1],\n",
       " [0.7346960968426424, 0.8103765894049038, 0],\n",
       " [0.84804181942037, 0.22331762671416083, 1],\n",
       " [0.576744258137402, 0.9500749493451289, 0],\n",
       " [0.6993116936688125, 0.6172721034109061, 1],\n",
       " [0.6148102005958451, 0.7397341368659701, 0],\n",
       " [0.4350992776568848, 0.2988339400105809, 0],\n",
       " [0.0, 0.7120978146321194, 1],\n",
       " [0.718958543589104, 0.7026086331188076, 0],\n",
       " [0.1947381059266071, 0.5931599045711329, 1],\n",
       " [0.21121984926645726, 0.6006108347419579, 1],\n",
       " [0.4106186953471298, 0.39412107546687464, 0],\n",
       " [0.24515010437372145, 0.42479339903531727, 0],\n",
       " [0.04377180953454055, 0.05393416950516847, 0],\n",
       " [0.9263818256280657, 0.27644598121005864, 1],\n",
       " [0.02278672379720245, 0.42724510451942105, 0],\n",
       " [0.8784196713445059, 0.7831017490376582, 0],\n",
       " [0.03359880949043605, 0.8422861297984895, 1],\n",
       " [0.8595362928239771, 0.7410983211310825, 0],\n",
       " [0.1730977291664622, 0.8558870352485052, 0],\n",
       " [0.9695736057121823, 0.8137788614896502, 0],\n",
       " [0.30769752841268694, 0.9422404049326003, 1],\n",
       " [0.8651974530729849, 0.44896914523204734, 1],\n",
       " [0.7012831066183864, 0.09435355135439201, 1],\n",
       " [0.3204195871113978, 0.6797472650865221, 1],\n",
       " [0.5686399104468249, 0.13559308377405413, 1],\n",
       " [0.05653272533476377, 0.9319949630499952, 1],\n",
       " [0.9986761338237182, 0.00014729770059271208, 1],\n",
       " [0.616389055477422, 0.7814022688152078, 0],\n",
       " [0.7126277539467364, 0.856007826982109, 0],\n",
       " [0.7017560087545671, 0.5608426687791588, 0],\n",
       " [0.6934507512646786, 0.9018374942046716, 0],\n",
       " [0.3056181606049068, 0.6178439955898208, 1],\n",
       " [0.1535666182688037, 0.986798853048522, 1],\n",
       " [0.10118370860770691, 0.29633688512839085, 0],\n",
       " [0.5769944915513314, 0.778953119181212, 0],\n",
       " [0.6437275141334602, 0.33387235768463913, 1],\n",
       " [0.29665143686087775, 0.8889027030229368, 1],\n",
       " [0.10593269700844694, 0.11871086693308507, 0],\n",
       " [0.3418573379428349, 0.17624055027897778, 0],\n",
       " [0.6062806142380751, 0.7376580279692071, 0],\n",
       " [0.9498361685101752, 0.4160715456899108, 1],\n",
       " [0.012092644216955429, 0.008990051663268134, 0],\n",
       " [0.237771196545247, 0.3721948199173351, 0],\n",
       " [0.3284344350175274, 0.6860018346774174, 0],\n",
       " [0.4391383163488464, 0.6250634865504203, 1],\n",
       " [0.6801098962306098, 0.3280941193902801, 1],\n",
       " [0.05689141944796696, 0.3638349635757969, 0],\n",
       " [0.6818284057945273, 0.5795645848765004, 0],\n",
       " [0.431244706079911, 0.20893798465858154, 0],\n",
       " [0.9424776538024876, 0.3647010645814285, 1],\n",
       " [0.08194323571766067, 0.2788936520899794, 0],\n",
       " [0.2747912440287685, 0.8030643979028633, 1],\n",
       " [0.25621077883922694, 0.2923548588792895, 0],\n",
       " [0.1766809522703143, 0.13873377205644957, 0],\n",
       " [0.42286442055871193, 0.1249935246954008, 0],\n",
       " [0.5995177572203649, 0.9103648232211308, 0],\n",
       " [0.3956782632673719, 0.9374165176249082, 1],\n",
       " [0.8424460906772947, 0.2669484933601059, 1],\n",
       " [0.682255447138993, 0.0002669645198120153, 1],\n",
       " [0.5948415959323848, 0.7410896612668946, 0],\n",
       " [0.023010342901886843, 0.7911741602966902, 1],\n",
       " [0.7912948135845499, 0.7778562024465774, 0],\n",
       " [0.5908127185461747, 0.8808770371477255, 0],\n",
       " [0.6712966146087785, 0.12905960833652938, 1],\n",
       " [0.6361884087074566, 0.7424472226550815, 0],\n",
       " [0.6898900749922314, 0.6862684325093147, 1],\n",
       " [0.8682085498255773, 0.12723917950325547, 1],\n",
       " [0.009520793935621788, 0.320736922351325, 0],\n",
       " [0.8790258442988577, 0.9981920957686854, 0],\n",
       " [0.5771264351003204, 0.090457795462961, 1],\n",
       " [0.06318481426462198, 0.629423052529729, 1],\n",
       " [0.7673408349561135, 0.0202517886204223, 1],\n",
       " [0.8000603397424477, 0.3753900975452288, 1],\n",
       " [0.7053074454864084, 0.973449176598133, 0],\n",
       " [0.360365558049042, 0.327421799812359, 0],\n",
       " [0.979008044166874, 0.025716980718590028, 1],\n",
       " [0.5565202881139023, 0.9557060435537134, 0],\n",
       " [0.1773993518964154, 0.3855316692887637, 0],\n",
       " [0.07502189550691271, 0.37329432754268904, 0],\n",
       " [0.06356021744400438, 0.1607152714456646, 0],\n",
       " [0.39301054087713827, 0.6463664513856981, 1],\n",
       " [0.29312862841140136, 0.21371245413168988, 0],\n",
       " [0.06387607558351328, 0.9940695912052763, 1],\n",
       " [0.5747539696810287, 0.37942183816512176, 1],\n",
       " [0.6590061621002224, 0.24448975252468255, 1],\n",
       " [0.40954244869768813, 0.0412702592759875, 0],\n",
       " [0.8792902621834623, 0.4096225632354611, 1],\n",
       " [0.25096213169342335, 0.6102698554804441, 1],\n",
       " [0.8081337097490392, 0.038431447934504766, 1],\n",
       " [0.9215017579466416, 0.8213820454326375, 0],\n",
       " [0.13775770861117528, 0.8366423035636377, 1],\n",
       " [0.8214492035317719, 0.3999129607703611, 1],\n",
       " [0.90970376951265, 0.8679375230353158, 0],\n",
       " [0.16597162566768311, 0.5811582451713807, 1],\n",
       " [0.023165675923703475, 0.9210645990523568, 1],\n",
       " [0.3147438901479098, 0.9895899922021524, 1],\n",
       " [0.3756593865940321, 0.05494948845703232, 0],\n",
       " [0.5613286902196264, 0.08440028483544064, 1],\n",
       " [0.387195903705213, 0.1719094665772757, 0],\n",
       " [0.3162376807732718, 0.17848622029123565, 0],\n",
       " [0.2679671395603329, 0.7156899011043307, 1],\n",
       " [0.7217222001646516, 0.05238349772704733, 1],\n",
       " [0.8103779103075781, 0.06609496302194884, 1],\n",
       " [0.1395252788173169, 0.8149190916420227, 1],\n",
       " [0.6805218471761036, 0.599130857870459, 0],\n",
       " [0.7160333960397198, 0.709430240958416, 0],\n",
       " [0.3998218994055117, 0.02229622058301541, 0],\n",
       " [0.847249082431152, 0.12635279995210946, 1],\n",
       " [0.40269402831083445, 0.9113005435779303, 1],\n",
       " [0.1879929299944784, 0.08990352925528253, 0],\n",
       " [0.30940253701223547, 0.10793097920211521, 0],\n",
       " [0.5912849106468049, 0.0033223953951061727, 1],\n",
       " [0.2682793050408822, 0.6128399558397831, 1],\n",
       " [0.9246196804001803, 0.008262289229095936, 1],\n",
       " [0.725451277320764, 0.7390438844496658, 0],\n",
       " [0.1746733449584577, 0.5689906777405569, 1],\n",
       " [0.5781394721436838, 0.09522403236595674, 1],\n",
       " [0.6327302545478426, 0.9061959012258296, 0],\n",
       " [0.7102923526051013, 0.741416545866449, 0],\n",
       " [0.6574797196240851, 0.8334862868262276, 0],\n",
       " [0.37495785058965364, 0.762042000481523, 1],\n",
       " [0.20145088329667585, 0.675899527247874, 1],\n",
       " [0.129388106709136, 0.23604810193129072, 0],\n",
       " [0.8652893248543029, 0.672106676197155, 0],\n",
       " [0.23273825414466118, 0.004470853319645486, 0],\n",
       " [0.692794999793047, 0.34445041592472914, 1],\n",
       " [0.9382254182362926, 0.030155804502879472, 1],\n",
       " [0.9026376521654736, 0.8412889947288369, 0],\n",
       " [0.5679594553061492, 0.15525509959977254, 1],\n",
       " [0.5637900555396578, 0.32440578461542147, 1],\n",
       " [0.646540296917805, 0.05332312632030794, 1],\n",
       " [0.27613739406406135, 0.16707946679742508, 0],\n",
       " [0.40276002046140236, 0.1424070758187244, 0],\n",
       " [0.7084403497001508, 0.6898855937610628, 0],\n",
       " [0.5994842619817166, 0.2632426678828537, 1],\n",
       " [0.851016748339924, 0.9387869894645448, 0],\n",
       " [0.8397167976435661, 0.07176936973197916, 1],\n",
       " [0.24841170275796287, 0.19709253590140474, 0],\n",
       " [0.020267384614131693, 0.7699164138599717, 1],\n",
       " [0.18432687058656433, 0.7211432086152915, 1],\n",
       " [0.6337575149983384, 0.5961093676786914, 0],\n",
       " [0.7617916148024949, 0.02642445738767356, 1],\n",
       " [0.07928859807519008, 0.07918918135285884, 0],\n",
       " [0.5524763729837222, 0.011294205684282934, 1],\n",
       " [0.24458431709961803, 0.2577272162504521, 0],\n",
       " [0.6175868582158598, 0.8724601826200424, 0],\n",
       " [0.19286838683274077, 0.3368196477708377, 0],\n",
       " [0.9112256325167088, 0.6596649095383676, 0],\n",
       " [0.8061968237533589, 0.13036323769275485, 1],\n",
       " [0.8873460119300779, 0.32295445072502266, 1],\n",
       " [0.7689856816856434, 0.23989675042306788, 1],\n",
       " [0.06802161156858733, 0.6746439809608762, 1],\n",
       " [0.2631628449482328, 0.4116933950967098, 0],\n",
       " [0.4086962147310401, 0.1856292192083487, 0],\n",
       " [0.5928080556286749, 0.9521019247598556, 0],\n",
       " [0.8127248774918316, 0.23927902754111804, 1],\n",
       " [0.8200890401041236, 0.8840058500863857, 0],\n",
       " [0.15056298924626937, 0.2605422132272154, 0],\n",
       " [0.19284443798626016, 0.11653044364674253, 0],\n",
       " [0.5580443486068757, 0.6711011973336563, 0],\n",
       " [0.695716689462239, 0.1757412436188425, 1],\n",
       " [0.2197429279905423, 0.8838805961121726, 1],\n",
       " [0.8713084415723925, 0.6054074672003541, 0],\n",
       " [0.19038127715084308, 0.5947447085528805, 1],\n",
       " [0.25542033597396485, 0.02296900570767937, 0],\n",
       " [0.43693303963859137, 0.7591429404740134, 1],\n",
       " [0.5974607932079888, 0.7181870663443083, 0],\n",
       " [0.9117160564621343, 0.2454182572342148, 1],\n",
       " [0.20796985031509396, 0.815839694593007, 1],\n",
       " [0.7485109183096802, 0.26690234467359203, 1],\n",
       " [0.7317001301315805, 0.7547186232712273, 0],\n",
       " [0.05112572019781547, 0.8773933509212053, 1],\n",
       " [0.23716784194482085, 0.871201265788723, 1],\n",
       " [0.033264095846976, 0.0, 0],\n",
       " [0.553615553657892, 0.36856633158658936, 1],\n",
       " [0.15267959604314862, 0.9736704336914054, 1],\n",
       " [0.35180827576557616, 0.9316638348108865, 1],\n",
       " [0.7848409812067956, 0.025176813332473312, 1],\n",
       " [0.5823078377740103, 0.06617550265881036, 1],\n",
       " [0.012884024381597498, 0.2851167472572724, 0],\n",
       " [0.13384878728783497, 0.04154871687465871, 0],\n",
       " [0.9313102071988827, 0.5785056845871899, 0],\n",
       " [0.8451657305966488, 0.8297691030556402, 0],\n",
       " [0.6531983691706812, 0.7504615689024597, 0],\n",
       " [0.8008864450737774, 0.5728870424827482, 0],\n",
       " [0.8475501788619636, 0.6414582863731698, 0],\n",
       " [0.08000935678659035, 0.28725245439381797, 0],\n",
       " [0.3076386848342737, 0.8792082508897437, 1],\n",
       " [0.10412257147371815, 0.16172070620632498, 0],\n",
       " [0.12997678014687797, 0.1066024764491766, 0],\n",
       " [0.18866698452629072, 0.6727694747979336, 0],\n",
       " [0.363741364829486, 0.9964172183204699, 1],\n",
       " [0.6233324702284183, 0.7812941931986286, 0],\n",
       " [0.4107563277546134, 0.8747542945657198, 1],\n",
       " [0.03445588141995686, 0.928633365261081, 1],\n",
       " [0.8868337016608177, 0.5905405476848522, 0],\n",
       " [0.16234549468882398, 0.77317128630414, 1],\n",
       " [0.9647623599337483, 0.2193314184486496, 1],\n",
       " [0.5799780566981473, 0.8549019334067216, 0],\n",
       " [0.15931420020383236, 0.5922914625908663, 1],\n",
       " [0.07783073468380484, 0.8952728667574456, 1],\n",
       " [0.32898773024551403, 0.641124965477876, 1],\n",
       " [0.5948113464408469, 0.21592031633122902, 1],\n",
       " [0.05026499601851837, 0.3756836342567713, 0],\n",
       " [0.809869271463795, 0.029396637323561993, 1],\n",
       " [0.13292293704196442, 0.7865704400977371, 1],\n",
       " [0.4171333972095337, 0.5942489216617391, 1],\n",
       " [0.19149567476997953, 0.05649939825289399, 0],\n",
       " [0.10863534630866988, 0.4321520890258365, 0],\n",
       " [0.5659675352508228, 0.2829876058049303, 1]]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = prepare_data(train_path)\n",
    "testset = prepare_data(validate_path)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "167\n",
      "168\n",
      "169\n",
      "170\n",
      "171\n",
      "172\n",
      "173\n",
      "174\n",
      "175\n",
      "176\n",
      "177\n",
      "178\n",
      "179\n",
      "180\n",
      "181\n",
      "182\n",
      "183\n",
      "184\n",
      "185\n",
      "186\n",
      "187\n",
      "188\n",
      "189\n",
      "190\n",
      "191\n",
      "192\n",
      "193\n",
      "194\n",
      "195\n",
      "196\n",
      "197\n",
      "198\n",
      "199\n",
      "200\n",
      "201\n",
      "202\n",
      "203\n",
      "204\n",
      "205\n",
      "206\n",
      "207\n",
      "208\n",
      "209\n",
      "210\n",
      "211\n",
      "212\n",
      "213\n",
      "214\n",
      "215\n",
      "216\n",
      "217\n",
      "218\n",
      "219\n",
      "220\n",
      "221\n",
      "222\n",
      "223\n",
      "224\n",
      "225\n",
      "226\n",
      "227\n",
      "228\n",
      "229\n",
      "230\n",
      "231\n",
      "232\n",
      "233\n",
      "234\n",
      "235\n",
      "236\n",
      "237\n",
      "238\n",
      "239\n",
      "240\n",
      "241\n",
      "242\n",
      "243\n",
      "244\n",
      "245\n",
      "246\n",
      "247\n",
      "248\n",
      "249\n",
      "250\n",
      "251\n",
      "252\n",
      "253\n",
      "254\n",
      "255\n",
      "256\n",
      "257\n",
      "258\n",
      "259\n",
      "260\n",
      "261\n",
      "262\n",
      "263\n",
      "264\n",
      "265\n",
      "266\n",
      "267\n",
      "268\n",
      "269\n",
      "270\n",
      "271\n",
      "272\n",
      "273\n",
      "274\n",
      "275\n",
      "276\n",
      "277\n",
      "278\n",
      "279\n",
      "280\n",
      "281\n",
      "282\n",
      "283\n",
      "284\n",
      "285\n",
      "286\n",
      "287\n",
      "288\n",
      "289\n",
      "290\n",
      "291\n",
      "292\n",
      "293\n",
      "294\n",
      "295\n",
      "296\n",
      "297\n",
      "298\n",
      "299\n",
      "300\n",
      "301\n",
      "302\n",
      "303\n",
      "304\n",
      "305\n",
      "306\n",
      "307\n",
      "308\n",
      "309\n",
      "310\n",
      "311\n",
      "312\n",
      "313\n",
      "314\n",
      "315\n",
      "316\n",
      "317\n",
      "318\n",
      "319\n",
      "320\n",
      "321\n",
      "322\n",
      "323\n",
      "324\n",
      "325\n",
      "326\n",
      "327\n",
      "328\n",
      "329\n",
      "330\n",
      "331\n",
      "332\n",
      "333\n",
      "334\n",
      "335\n",
      "336\n",
      "337\n",
      "338\n",
      "339\n",
      "340\n",
      "341\n",
      "342\n",
      "343\n",
      "344\n",
      "345\n",
      "346\n",
      "347\n",
      "348\n",
      "349\n",
      "350\n",
      "351\n",
      "352\n",
      "353\n",
      "354\n",
      "355\n",
      "356\n",
      "357\n",
      "358\n",
      "359\n",
      "360\n",
      "361\n",
      "362\n",
      "363\n",
      "364\n",
      "365\n",
      "366\n",
      "367\n",
      "368\n",
      "369\n",
      "370\n",
      "371\n",
      "372\n",
      "373\n",
      "374\n",
      "375\n",
      "376\n",
      "377\n",
      "378\n",
      "379\n",
      "380\n",
      "381\n",
      "382\n",
      "383\n",
      "384\n",
      "385\n",
      "386\n",
      "387\n",
      "388\n",
      "389\n",
      "390\n",
      "391\n",
      "392\n",
      "393\n",
      "394\n",
      "395\n",
      "396\n",
      "397\n",
      "398\n",
      "399\n",
      "400\n",
      "401\n",
      "402\n",
      "403\n",
      "404\n",
      "405\n",
      "406\n",
      "407\n",
      "408\n",
      "409\n",
      "410\n",
      "411\n",
      "412\n",
      "413\n",
      "414\n",
      "415\n",
      "416\n",
      "417\n",
      "418\n",
      "419\n",
      "420\n",
      "421\n",
      "422\n",
      "423\n",
      "424\n",
      "425\n",
      "426\n",
      "427\n",
      "428\n",
      "429\n",
      "430\n",
      "431\n",
      "432\n",
      "433\n",
      "434\n",
      "435\n",
      "436\n",
      "437\n",
      "438\n",
      "439\n",
      "440\n",
      "441\n",
      "442\n",
      "443\n",
      "444\n",
      "445\n",
      "446\n",
      "447\n",
      "448\n",
      "449\n",
      "450\n",
      "451\n",
      "452\n",
      "453\n",
      "454\n",
      "455\n",
      "456\n",
      "457\n",
      "458\n",
      "459\n",
      "460\n",
      "461\n",
      "462\n",
      "463\n",
      "464\n",
      "465\n",
      "466\n",
      "467\n",
      "468\n",
      "469\n",
      "470\n",
      "471\n",
      "472\n",
      "473\n",
      "474\n",
      "475\n",
      "476\n",
      "477\n",
      "478\n",
      "479\n",
      "480\n",
      "481\n",
      "482\n",
      "483\n",
      "484\n",
      "485\n",
      "486\n",
      "487\n",
      "488\n",
      "489\n",
      "490\n",
      "491\n",
      "492\n",
      "493\n",
      "494\n",
      "495\n",
      "496\n",
      "497\n",
      "498\n",
      "499\n",
      "500\n",
      "501\n",
      "502\n",
      "503\n",
      "504\n",
      "505\n",
      "506\n",
      "507\n",
      "508\n",
      "509\n",
      "510\n",
      "511\n",
      "512\n",
      "513\n",
      "514\n",
      "515\n",
      "516\n",
      "517\n",
      "518\n",
      "519\n",
      "520\n",
      "521\n",
      "522\n",
      "523\n",
      "524\n",
      "525\n",
      "526\n",
      "527\n",
      "528\n",
      "529\n",
      "530\n",
      "531\n",
      "532\n",
      "533\n",
      "534\n",
      "535\n",
      "536\n",
      "537\n",
      "538\n",
      "539\n",
      "540\n",
      "541\n",
      "542\n",
      "543\n",
      "544\n",
      "545\n",
      "546\n",
      "547\n",
      "548\n",
      "549\n",
      "550\n",
      "551\n",
      "552\n",
      "553\n",
      "554\n",
      "555\n",
      "556\n",
      "557\n",
      "558\n",
      "559\n",
      "560\n",
      "561\n",
      "562\n",
      "563\n",
      "564\n",
      "565\n",
      "566\n",
      "567\n",
      "568\n",
      "569\n",
      "570\n",
      "571\n",
      "572\n",
      "573\n",
      "574\n",
      "575\n",
      "576\n",
      "577\n",
      "578\n",
      "579\n",
      "580\n",
      "581\n",
      "582\n",
      "583\n",
      "584\n",
      "585\n",
      "586\n",
      "587\n",
      "588\n",
      "589\n",
      "590\n",
      "591\n",
      "592\n",
      "593\n",
      "594\n",
      "595\n",
      "596\n",
      "597\n",
      "598\n",
      "599\n",
      "600\n",
      "601\n",
      "602\n",
      "603\n",
      "604\n",
      "605\n",
      "606\n",
      "607\n",
      "608\n",
      "609\n",
      "610\n",
      "611\n",
      "612\n",
      "613\n",
      "614\n",
      "615\n",
      "616\n",
      "617\n",
      "618\n",
      "619\n",
      "620\n",
      "621\n",
      "622\n",
      "623\n",
      "624\n",
      "625\n",
      "626\n",
      "627\n",
      "628\n",
      "629\n",
      "630\n",
      "631\n",
      "632\n",
      "633\n",
      "634\n",
      "635\n",
      "636\n",
      "637\n",
      "638\n",
      "639\n",
      "640\n",
      "641\n",
      "642\n",
      "643\n",
      "644\n",
      "645\n",
      "646\n",
      "647\n",
      "648\n",
      "649\n",
      "650\n",
      "651\n",
      "652\n",
      "653\n",
      "654\n",
      "655\n",
      "656\n",
      "657\n",
      "658\n",
      "659\n",
      "660\n",
      "661\n",
      "662\n",
      "663\n",
      "664\n",
      "665\n",
      "666\n",
      "667\n",
      "668\n",
      "669\n",
      "670\n",
      "671\n",
      "672\n",
      "673\n",
      "674\n",
      "675\n",
      "676\n",
      "677\n",
      "678\n",
      "679\n",
      "680\n",
      "681\n",
      "682\n",
      "683\n",
      "684\n",
      "685\n",
      "686\n",
      "687\n",
      "688\n",
      "689\n",
      "690\n",
      "691\n",
      "692\n",
      "693\n",
      "694\n",
      "695\n",
      "696\n",
      "697\n",
      "698\n",
      "699\n",
      "700\n",
      "701\n",
      "702\n",
      "703\n",
      "704\n",
      "705\n",
      "706\n",
      "707\n",
      "708\n",
      "709\n",
      "710\n",
      "711\n",
      "712\n",
      "713\n",
      "714\n",
      "715\n",
      "716\n",
      "717\n",
      "718\n",
      "719\n",
      "720\n",
      "721\n",
      "722\n",
      "723\n",
      "724\n",
      "725\n",
      "726\n",
      "727\n",
      "728\n",
      "729\n",
      "730\n",
      "731\n",
      "732\n",
      "733\n",
      "734\n",
      "735\n",
      "736\n",
      "737\n",
      "738\n",
      "739\n",
      "740\n",
      "741\n",
      "742\n",
      "743\n",
      "744\n",
      "745\n",
      "746\n",
      "747\n",
      "748\n",
      "749\n",
      "750\n",
      "751\n",
      "752\n",
      "753\n",
      "754\n",
      "755\n",
      "756\n",
      "757\n",
      "758\n",
      "759\n",
      "760\n",
      "761\n",
      "762\n",
      "763\n",
      "764\n",
      "765\n",
      "766\n",
      "767\n",
      "768\n",
      "769\n",
      "770\n",
      "771\n",
      "772\n",
      "773\n",
      "774\n",
      "775\n",
      "776\n",
      "777\n",
      "778\n",
      "779\n",
      "780\n",
      "781\n",
      "782\n",
      "783\n",
      "784\n",
      "785\n",
      "786\n",
      "787\n",
      "788\n",
      "789\n",
      "790\n",
      "791\n",
      "792\n",
      "793\n",
      "794\n",
      "795\n",
      "796\n",
      "797\n",
      "798\n",
      "799\n",
      "Accuracy: 95.1219512195122\n"
     ]
    }
   ],
   "source": [
    "#lr = 0.0002, batch_size = 10, nrof_epochs = 800 => 95%\n",
    "# relu \\w l_rate = 0.001 & n_epochs = 800 => 97%\n",
    "learning_rate = 0.0002\n",
    "nrof_epochs = 800\n",
    "nrof_hidden_neurons = 10 #Nr of neurons in one hidden layer\n",
    "batch_size = 10 #For training with batches\n",
    "results = evaluate_accuracy(dataset, testset, algorithm, learning_rate, nrof_epochs, nrof_hidden_neurons, batch_size)\n",
    "print('Accuracy: %s' % results[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### h) Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(list(range(nrof_epochs - 1)), results[1][1][1:])\n",
    "ax.set(xlabel='epochs', ylabel='loss',\n",
    "       title='Training loss')\n",
    "ax.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### i) Confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[39, 39], [2, 2]]\n"
     ]
    }
   ],
   "source": [
    "true_pos = 0\n",
    "true_neg = 0\n",
    "false_pos = 0\n",
    "false_neg = 0\n",
    "\n",
    "for i in range(len(testset)):\n",
    "    if testset[i][2] == results[1][0][i]:\n",
    "        if testset[i][2] == 1:\n",
    "            true_pos += 1\n",
    "        else:\n",
    "            true_neg += 1\n",
    "    else:\n",
    "        if testset[i][2] == 1:\n",
    "            false_neg += 1\n",
    "        else:\n",
    "            false_pos += 1\n",
    "\n",
    "matrix = [[true_pos, true_neg], [false_pos, false_neg]]\n",
    "\n",
    "print(matrix)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Skeleton code - UNUSED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "ENblnfHo9ZS5"
   },
   "outputs": [],
   "source": [
    "# To DO\n",
    "# implement the activation and its derivative\n",
    "\n",
    "#ReLU\n",
    "def activation(x):\n",
    "    return max(0, x)\n",
    "\n",
    "def activation_derivative(x):\n",
    "    if x < 0:\n",
    "        return 0\n",
    "    if x > 0:\n",
    "        return 1\n",
    "\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "fkinwPxR-tzB"
   },
   "outputs": [],
   "source": [
    "# To DO \n",
    "# implement the loss its derivative\n",
    "def compute_loss(predicted_op, true_y):\n",
    "    loss = 0\n",
    "    return loss\n",
    "\n",
    "def loss_derivative(x):\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "SbtyV0Bh7ZmC"
   },
   "outputs": [],
   "source": [
    "# TO DO \n",
    "# organize the training data into batches\n",
    "def make_batches(x_train, y_train, batch_size):\n",
    "    \n",
    "    return x_batches, y_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "6WPxFybEy0ML"
   },
   "outputs": [],
   "source": [
    "# TO DO \n",
    "def forward_pass(x_batch, y_batch):\n",
    "\n",
    "  return predicted_op\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "5odxcSaWzP6c"
   },
   "outputs": [],
   "source": [
    "# TO DO \n",
    "def backward_pass(loss):\n",
    "\n",
    "  return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "Pxo61LUjzlB_"
   },
   "outputs": [],
   "source": [
    "#TO DO\n",
    "def update_weights(weights, grads, lr):\n",
    "\n",
    "  return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "zxDJLFXsE1-O"
   },
   "outputs": [],
   "source": [
    "# To Do\n",
    "# define you stopping criteria for training\n",
    "def stopping_criteria():\n",
    "  stop = False\n",
    "  return stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "uv6XNf5WETxn"
   },
   "outputs": [],
   "source": [
    "# TO DO\n",
    "# Define the hyperparameters. Add any additional hyperparameters you might need\n",
    "lr = 0  # learning rate\n",
    "batch_size = 0\n",
    "num_epochs = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "oU6_R9PZEnFr"
   },
   "outputs": [],
   "source": [
    "def train(x_train, y_train, lr, batch_size, num_epochs):\n",
    "\n",
    "  while not stopping_criteria():\n",
    "      # TO DO \n",
    "      # get a batch from the data\n",
    "      x_batch, y_batch = [], []\n",
    "\n",
    "      # forward the batch through the network layers\n",
    "      predicted_op = forward_pass(x_batch)\n",
    "\n",
    "      # compute the loss\n",
    "      loss = compute_loss(predicted_op, y_batch)\n",
    "\n",
    "      # perform backward pass\n",
    "      grads = backward_pass(loss)\n",
    "\n",
    "      # update the weights of the network\n",
    "      update_weights(network, grads, lr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9RrDycnT_Nti"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "MLP.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
